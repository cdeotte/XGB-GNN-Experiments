{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9e0bba-b79e-4325-bfe3-f82bf41f7811",
   "metadata": {},
   "source": [
    "# Build XGB using GNN embeddings\n",
    "In this notebook we build a simple GNN model and combine it with XGB model to see if GNN improves XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711e5b1-0f28-471f-b889-1cd46fc4c735",
   "metadata": {},
   "source": [
    "# Simple GNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401f6dcc-853f-49f9-a70e-ccc18deb1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"7\")\n",
    "\n",
    "LOAD = 2\n",
    "VER=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba98df61-5aac-44e3-9c8f-12c1e5d0a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02213d78-7c61-4579-ab9d-f95921d44167",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_features = pd.read_parquet(f\"data/author_features_v{LOAD}.pqt\")\n",
    "author_targets = pd.read_parquet(f\"data/author_targets_v{LOAD}.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7852e8aa-82c1-4e65-aef5-84edac567a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>coauthor_ids</th>\n",
       "      <th>coauthor_counts</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 2, 4, 5, 6, 7, 8]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 5, 6, 7, 8, 2905, 2819, 2906, 2907, ...</td>\n",
       "      <td>[1, 1, 1, 4, 4, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Author_ID  target                                       coauthor_ids  \\\n",
       "0          0       0                                                 []   \n",
       "1          1       0                              [2, 3, 4, 5, 6, 7, 8]   \n",
       "2          2       0                              [1, 3, 4, 5, 6, 7, 8]   \n",
       "3          3       0                              [1, 2, 4, 5, 6, 7, 8]   \n",
       "4          4       1  [1, 2, 3, 5, 6, 7, 8, 2905, 2819, 2906, 2907, ...   \n",
       "\n",
       "                                     coauthor_counts  degree  \n",
       "0                                                 []       0  \n",
       "1                              [1, 1, 1, 1, 1, 1, 1]       7  \n",
       "2                              [1, 1, 1, 1, 1, 1, 1]       7  \n",
       "3                              [1, 1, 1, 1, 1, 1, 1]       7  \n",
       "4  [1, 1, 1, 4, 4, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, ...      38  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdcbcf42-a74e-4ec0-8310-9803549f7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(author_features)\n",
    "assert author_features[\"Author_ID\"].is_unique\n",
    "\n",
    "id2idx = pd.Series(np.arange(N), index=author_features[\"Author_ID\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6aa8132-b791-4d87-8777-1ef826b2fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG TRANSFORM FOR NN\n",
    "for c in [\n",
    "    \"n_coauthors\",\n",
    "    \"total_collaborations\",\n",
    "    \"total_papers\",\n",
    "    \"total_citations\",\n",
    "    \"citations_last_3y\",\n",
    "    \"max_citations_single_paper\",\n",
    "]:\n",
    "    author_features[f\"log_{c}\"] = np.log1p(author_features[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c44f22-5cd7-49bf-ba1a-1e9159d3660b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: ['n_coauthors', 'total_collaborations', 'avg_collab_strength', 'max_collab_strength', 'collab_entropy', 'total_papers', 'papers_last_1y', 'papers_last_3y', 'days_since_last_paper', 'top_category_frac', 'category_entropy', 'total_citations', 'avg_citations_per_paper', 'max_citations_single_paper', 'citations_last_3y', 'log_n_coauthors', 'log_total_collaborations', 'log_total_papers', 'log_total_citations', 'log_citations_last_3y', 'log_max_citations_single_paper']\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"target\"\n",
    "\n",
    "BASE_FEATURES = [\n",
    "    # --------------------\n",
    "    # Collaboration graph\n",
    "    # --------------------\n",
    "    \"n_coauthors\",\n",
    "    \"total_collaborations\",\n",
    "    \"avg_collab_strength\",\n",
    "    \"max_collab_strength\",\n",
    "    \"collab_entropy\",\n",
    "\n",
    "    # --------------------\n",
    "    # Productivity / recency\n",
    "    # --------------------\n",
    "    \"total_papers\",\n",
    "    \"papers_last_1y\",\n",
    "    \"papers_last_3y\",\n",
    "    \"days_since_last_paper\",\n",
    "\n",
    "    # --------------------\n",
    "    # Topic specialization\n",
    "    # --------------------\n",
    "    #\"top_category\",          # categorical-as-ordinal (needs encoding for NN)\n",
    "    \"top_category_frac\",\n",
    "    \"category_entropy\",\n",
    "\n",
    "    # --------------------\n",
    "    # Citation impact\n",
    "    # --------------------\n",
    "    \"total_citations\",\n",
    "    \"avg_citations_per_paper\",\n",
    "    \"max_citations_single_paper\",\n",
    "    \"citations_last_3y\",\n",
    "]\n",
    "\n",
    "EXTRA_FEATURES = [\n",
    "    \"log_n_coauthors\",\n",
    "    \"log_total_collaborations\",\n",
    "    \"log_total_papers\",\n",
    "    \"log_total_citations\",\n",
    "    \"log_citations_last_3y\",\n",
    "    \"log_max_citations_single_paper\",\n",
    "]\n",
    "\n",
    "FEATURES = [c for c in (BASE_FEATURES + EXTRA_FEATURES) if c in author_features.columns]\n",
    "print(\"Using features:\", FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83186e65-9524-420f-bd46-48e706fe101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = author_features[FEATURES].astype(np.float32).values\n",
    "y = author_features[TARGET].astype(np.int64).values\n",
    "\n",
    "# standardize\n",
    "X_mean = X.mean(axis=0, keepdims=True)\n",
    "X_std  = X.std(axis=0, keepdims=True) + 1e-6\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "x = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4982109-f4ff-4fad-9d3e-f9fe76e4b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure coauthor lists align to author_features order\n",
    "author_targets_aligned = author_targets.set_index(\"Author_ID\").loc[author_features[\"Author_ID\"]].reset_index()\n",
    "\n",
    "src_list = []\n",
    "dst_list = []\n",
    "w_list   = []\n",
    "\n",
    "for i, (co_ids, co_cts) in enumerate(\n",
    "    zip(author_targets_aligned[\"coauthor_ids\"], author_targets_aligned[\"coauthor_counts\"])\n",
    "):\n",
    "    # skip isolated authors\n",
    "    if len(co_ids) == 0:\n",
    "        continue\n",
    "\n",
    "    js = id2idx.loc[co_ids].values.astype(np.int64)\n",
    "    cts = np.asarray(co_cts, dtype=np.float32)\n",
    "\n",
    "    src_list.append(np.full(len(js), i, dtype=np.int64))\n",
    "    dst_list.append(js)\n",
    "    w_list.append(cts)\n",
    "\n",
    "src = np.concatenate(src_list) if src_list else np.empty(0, dtype=np.int64)\n",
    "dst = np.concatenate(dst_list) if dst_list else np.empty(0, dtype=np.int64)\n",
    "w   = np.concatenate(w_list)   if w_list   else np.empty(0, dtype=np.float32)\n",
    "\n",
    "edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "edge_weight = torch.tensor(w, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74fd37b-4d6d-4e8f-9a11-14e98d359388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes: 143691\n",
      "Num edges (directed): 11182120\n"
     ]
    }
   ],
   "source": [
    "print(\"Num nodes:\", N)\n",
    "print(\"Num edges (directed):\", edge_index.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f6a8745-a02e-4129-a67f-348b1ef88680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "data.edge_weight = edge_weight  # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c91fccd-cfeb-4150-be47-714acfe39530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.lin   = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, return_emb=False):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "\n",
    "        if return_emb:\n",
    "            return h\n",
    "\n",
    "        out = self.lin(h).squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf992892-9326-48cb-9b6a-37779886e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = data.to(device)\n",
    "\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "N = data.num_nodes\n",
    "EMB_DIM = 128\n",
    "\n",
    "# OOF storage (optional, for diagnostics)\n",
    "oof_gnn = np.zeros(N, dtype=np.float32)\n",
    "oof_embs  = np.zeros((N, EMB_DIM), dtype=np.float32)\n",
    "\n",
    "fold_aucs   = []\n",
    "fold_states = []        \n",
    "fold_splits = []        \n",
    "\n",
    "def train_one_fold(train_idx, val_idx, epochs=300, lr=5e-3, wd=1e-4):\n",
    "    model = GraphSAGE(\n",
    "        in_dim=data.x.size(1),\n",
    "        hidden_dim=EMB_DIM,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    train_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    val_mask   = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "    train_mask[torch.tensor(train_idx, device=device)] = True\n",
    "    val_mask[torch.tensor(val_idx, device=device)] = True\n",
    "\n",
    "    y_float = data.y.float()\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits[train_mask],\n",
    "            y_float[train_mask]\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # validation AUC\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_probs = torch.sigmoid(logits[val_mask]).cpu().numpy()\n",
    "            val_true  = data.y[val_mask].cpu().numpy()\n",
    "            auc = roc_auc_score(val_true, val_probs)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = {\n",
    "                k: v.detach().cpu().clone()\n",
    "                for k, v in model.state_dict().items()\n",
    "            }\n",
    "\n",
    "        if epoch % 30 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"  epoch {epoch:3d} | \"\n",
    "                f\"loss {loss.item():.4f} | \"\n",
    "                f\"val AUC {auc:.5f} | \"\n",
    "                f\"best {best_auc:.5f}\"\n",
    "            )\n",
    "\n",
    "    # restore best model\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        val_probs = torch.sigmoid(logits[val_mask]).cpu().numpy()\n",
    "\n",
    "        emb = model(data.x, data.edge_index, return_emb=True)\n",
    "        val_emb = emb[val_mask].cpu().numpy()\n",
    "\n",
    "    return best_auc, best_state, val_probs, val_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f2f785-a136-433f-9c72-5e323fee31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### FOLD 1 #####\n",
      "  epoch   1 | loss 0.6937 | val AUC 0.56596 | best 0.56596\n",
      "  epoch  30 | loss 0.5739 | val AUC 0.74487 | best 0.74658\n",
      "  epoch  60 | loss 0.5698 | val AUC 0.75085 | best 0.75085\n",
      "  epoch  90 | loss 0.5680 | val AUC 0.74948 | best 0.75173\n",
      "  epoch 120 | loss 0.5655 | val AUC 0.75151 | best 0.75211\n",
      "  epoch 150 | loss 0.5641 | val AUC 0.75170 | best 0.75228\n",
      "  epoch 180 | loss 0.5626 | val AUC 0.75061 | best 0.75343\n",
      "  epoch 210 | loss 0.5614 | val AUC 0.75155 | best 0.75343\n",
      "  epoch 240 | loss 0.5603 | val AUC 0.75213 | best 0.75343\n",
      "  epoch 270 | loss 0.5592 | val AUC 0.75101 | best 0.75343\n",
      "  epoch 300 | loss 0.5596 | val AUC 0.75169 | best 0.75343\n",
      "Fold 1 best AUC: 0.75343\n",
      "\n",
      "##### FOLD 2 #####\n",
      "  epoch   1 | loss 0.7109 | val AUC 0.46190 | best 0.46190\n",
      "  epoch  30 | loss 0.5742 | val AUC 0.74590 | best 0.74590\n",
      "  epoch  60 | loss 0.5695 | val AUC 0.74772 | best 0.75012\n",
      "  epoch  90 | loss 0.5675 | val AUC 0.74937 | best 0.75097\n",
      "  epoch 120 | loss 0.5653 | val AUC 0.75115 | best 0.75218\n",
      "  epoch 150 | loss 0.5634 | val AUC 0.75094 | best 0.75343\n",
      "  epoch 180 | loss 0.5627 | val AUC 0.75317 | best 0.75358\n",
      "  epoch 210 | loss 0.5615 | val AUC 0.75286 | best 0.75426\n",
      "  epoch 240 | loss 0.5602 | val AUC 0.75206 | best 0.75462\n",
      "  epoch 270 | loss 0.5599 | val AUC 0.75212 | best 0.75462\n",
      "  epoch 300 | loss 0.5587 | val AUC 0.75174 | best 0.75462\n",
      "Fold 2 best AUC: 0.75462\n",
      "\n",
      "##### FOLD 3 #####\n",
      "  epoch   1 | loss 0.6983 | val AUC 0.42199 | best 0.42199\n",
      "  epoch  30 | loss 0.5745 | val AUC 0.74997 | best 0.75031\n",
      "  epoch  60 | loss 0.5702 | val AUC 0.75369 | best 0.75385\n",
      "  epoch  90 | loss 0.5681 | val AUC 0.75517 | best 0.75517\n",
      "  epoch 120 | loss 0.5660 | val AUC 0.75466 | best 0.75571\n",
      "  epoch 150 | loss 0.5645 | val AUC 0.75562 | best 0.75730\n",
      "  epoch 180 | loss 0.5635 | val AUC 0.75580 | best 0.75779\n",
      "  epoch 210 | loss 0.5621 | val AUC 0.75664 | best 0.75788\n",
      "  epoch 240 | loss 0.5613 | val AUC 0.75649 | best 0.75792\n",
      "  epoch 270 | loss 0.5608 | val AUC 0.75726 | best 0.75792\n",
      "  epoch 300 | loss 0.5593 | val AUC 0.75617 | best 0.75792\n",
      "Fold 3 best AUC: 0.75792\n",
      "\n",
      "##### FOLD 4 #####\n",
      "  epoch   1 | loss 0.6970 | val AUC 0.50572 | best 0.50572\n",
      "  epoch  30 | loss 0.5744 | val AUC 0.75428 | best 0.75428\n",
      "  epoch  60 | loss 0.5702 | val AUC 0.75505 | best 0.75762\n",
      "  epoch  90 | loss 0.5679 | val AUC 0.75793 | best 0.75875\n",
      "  epoch 120 | loss 0.5658 | val AUC 0.75777 | best 0.75900\n",
      "  epoch 150 | loss 0.5646 | val AUC 0.76004 | best 0.76024\n",
      "  epoch 180 | loss 0.5632 | val AUC 0.75843 | best 0.76125\n",
      "  epoch 210 | loss 0.5620 | val AUC 0.75938 | best 0.76179\n",
      "  epoch 240 | loss 0.5610 | val AUC 0.75916 | best 0.76179\n",
      "  epoch 270 | loss 0.5607 | val AUC 0.75865 | best 0.76179\n",
      "  epoch 300 | loss 0.5589 | val AUC 0.75845 | best 0.76179\n",
      "Fold 4 best AUC: 0.76179\n",
      "\n",
      "##### FOLD 5 #####\n",
      "  epoch   1 | loss 0.6950 | val AUC 0.43643 | best 0.43643\n",
      "  epoch  30 | loss 0.5738 | val AUC 0.74901 | best 0.74901\n",
      "  epoch  60 | loss 0.5691 | val AUC 0.75070 | best 0.75294\n",
      "  epoch  90 | loss 0.5665 | val AUC 0.75295 | best 0.75389\n",
      "  epoch 120 | loss 0.5649 | val AUC 0.75251 | best 0.75454\n",
      "  epoch 150 | loss 0.5638 | val AUC 0.75426 | best 0.75544\n",
      "  epoch 180 | loss 0.5632 | val AUC 0.75381 | best 0.75556\n",
      "  epoch 210 | loss 0.5607 | val AUC 0.75449 | best 0.75577\n",
      "  epoch 240 | loss 0.5602 | val AUC 0.75477 | best 0.75634\n",
      "  epoch 270 | loss 0.5604 | val AUC 0.75419 | best 0.75634\n",
      "  epoch 300 | loss 0.5583 | val AUC 0.75361 | best 0.75634\n",
      "Fold 5 best AUC: 0.75634\n",
      "\n",
      "==============================\n",
      "Fold AUCs: ['0.75343', '0.75462', '0.75792', '0.76179', '0.75634']\n",
      "Mean AUC : 0.756818760232769\n",
      "OOF AUC  : 0.7556822341151668\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(N)), 1):\n",
    "    print(f\"\\n##### FOLD {fold} #####\")\n",
    "\n",
    "    auc, best_state, val_probs, val_emb = train_one_fold(\n",
    "        train_idx,\n",
    "        val_idx,\n",
    "        epochs=300,\n",
    "        lr=5e-3,\n",
    "        wd=1e-4\n",
    "    )\n",
    "\n",
    "    fold_aucs.append(auc)\n",
    "    fold_states.append(best_state)        # <-- SAVE MODEL\n",
    "    fold_splits.append((train_idx, val_idx))  # <-- SAVE SPLIT\n",
    "\n",
    "    oof_gnn[val_idx] = val_probs\n",
    "    oof_embs[val_idx]  = val_emb\n",
    "\n",
    "    print(f\"Fold {fold} best AUC: {auc:.5f}\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Fold AUCs:\", [f\"{a:.5f}\" for a in fold_aucs])\n",
    "print(\"Mean AUC :\", float(np.mean(fold_aucs)))\n",
    "print(\"OOF AUC  :\", float(roc_auc_score(data.y.cpu().numpy(), oof_gnn)))\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b50b2f-7795-4c0e-bfd4-cb11e2c0c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE GNN OOF\n",
    "np.save(f\"data/oof_preds_gnn_v{VER}\",oof_gnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b62199-e9a6-4b9a-80f8-e34aa88e918d",
   "metadata": {},
   "source": [
    "# Train XGB w/ GNN Embeddings\n",
    "We will now train XGB using GNN embeddings. This performs better than simple average of XGB and GNN probability predictions that was done in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cdec9b-dd29-4da3-bdd0-f8cd592c6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_estimators\": 1_000,\n",
    "    \"random_state\": 42,\n",
    "   \"early_stopping_rounds\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "524c52d3-345f-4b22-b368-2e08e4a4657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(f\"XGB version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6659fecf-d857-4772-a31c-ee8b9a019e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### STACK FOLD 1 #####\n",
      "Fold 1 AUC: 0.75610\n",
      "\n",
      "##### STACK FOLD 2 #####\n",
      "Fold 2 AUC: 0.75643\n",
      "\n",
      "##### STACK FOLD 3 #####\n",
      "Fold 3 AUC: 0.76022\n",
      "\n",
      "##### STACK FOLD 4 #####\n",
      "Fold 4 AUC: 0.76307\n",
      "\n",
      "##### STACK FOLD 5 #####\n",
      "Fold 5 AUC: 0.75704\n",
      "\n",
      "==============================\n",
      "Fold AUCs: ['0.75610', '0.75643', '0.76022', '0.76307', '0.75704']\n",
      "Mean AUC : 0.7585728593018518\n",
      "OOF AUC  : 0.7585639778208273\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "from cuml.decomposition import PCA\n",
    "\n",
    "# tabular features\n",
    "X_tab = author_features[FEATURES].values.astype(np.float32)\n",
    "y = author_features[TARGET].values.astype(np.int64)\n",
    "\n",
    "N = len(y)\n",
    "oof_preds = np.zeros(N, dtype=np.float32)\n",
    "fold_scores = []\n",
    "\n",
    "# IMPORTANT: reuse the SAME splits as GNN training\n",
    "for fold, ((train_idx, val_idx), gnn_state) in enumerate(\n",
    "    zip(fold_splits, fold_states), 1\n",
    "):\n",
    "    print(f\"\\n##### STACK FOLD {fold} #####\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load fold-specific GNN\n",
    "    # -----------------------------\n",
    "    gnn = GraphSAGE(\n",
    "        in_dim=data.x.size(1),\n",
    "        hidden_dim=EMB_DIM,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    gnn.load_state_dict({k: v.to(device) for k, v in gnn_state.items()})\n",
    "    gnn.eval()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract embeddings (same space)\n",
    "    # -----------------------------\n",
    "    with torch.no_grad():\n",
    "        emb_all = gnn(\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            return_emb=True\n",
    "        ).cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # -----------------------------\n",
    "    # PCA on embeddings (FIT = train only)\n",
    "    # -----------------------------\n",
    "    pca = PCA(n_components=8)\n",
    "    emb_train_pca = pca.fit_transform(emb_all[train_idx])\n",
    "    emb_val_pca   = pca.transform(emb_all[val_idx])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build stacked features\n",
    "    # -----------------------------\n",
    "    X_train = np.hstack([\n",
    "        X_tab[train_idx],\n",
    "        emb_train_pca\n",
    "    ])\n",
    "\n",
    "    X_val = np.hstack([\n",
    "        X_tab[val_idx],\n",
    "        emb_val_pca\n",
    "    ])\n",
    "\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Train stack XGB (regularized!)\n",
    "    # -----------------------------\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_preds = model.predict_proba(X_val)[:, 1]\n",
    "    oof_preds[val_idx] = rankdata(val_preds)\n",
    "\n",
    "    auc = roc_auc_score(y_val, val_preds)\n",
    "    fold_scores.append(auc)\n",
    "\n",
    "    print(f\"Fold {fold} AUC: {auc:.5f}\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Fold AUCs:\", [f\"{s:.5f}\" for s in fold_scores])\n",
    "print(\"Mean AUC :\", np.mean(fold_scores))\n",
    "print(\"OOF AUC  :\", roc_auc_score(y, oof_preds))\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1530029-76d7-4671-83ba-f3ea4a4d1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE GNN OOF\n",
    "np.save(f\"data/oof_preds_xgb_gnn_v{VER}\",oof_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7fd27-4483-44a7-adee-cabeca8013f5",
   "metadata": {},
   "source": [
    "# Compare to Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d86cc216-938f-4052-9d13-8d5d3bd7145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB OOF AUC  : 0.755080970524253\n",
      "GNN OOF AUC  : 0.7556822341151668\n",
      "BLEND OOF AUC  : 0.7576088669046419\n",
      "\n",
      "XGB w/ GNN emb  : 0.7585639778208273\n"
     ]
    }
   ],
   "source": [
    "oof_xgb = np.load(f\"data/oof_preds_xgb_v{LOAD}.npy\")\n",
    "print(\"XGB OOF AUC  :\", float(roc_auc_score(author_features[TARGET].values, oof_xgb)))\n",
    "\n",
    "oof_gnn = np.load(f\"data/oof_preds_gnn_v{VER}.npy\")\n",
    "print(\"GNN OOF AUC  :\", float(roc_auc_score(author_features[TARGET].values, oof_gnn)))\n",
    "\n",
    "blend = (oof_xgb + oof_gnn)/2.\n",
    "print(\"BLEND OOF AUC  :\", float(roc_auc_score(author_features[TARGET].values, blend)))\n",
    "\n",
    "oof_xgb_gnn = np.load(f\"data/oof_preds_xgb_gnn_v{VER}.npy\")\n",
    "print(\"XGB w/ GNN emb  :\", float(roc_auc_score(author_features[TARGET].values, oof_xgb_gnn)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
